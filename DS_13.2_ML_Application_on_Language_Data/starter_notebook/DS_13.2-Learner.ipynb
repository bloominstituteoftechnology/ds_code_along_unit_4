{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Install Prerequesties ###########################\n",
    "!pip install -r requirements.txt\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 1/4** - Extract Text Features and use them in Classification Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's load news group dataset. This dataset has 20 types of news groups\n",
    "\n",
    "# More detailed documentation about fetch newsgroup dataset is here - https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "data_newsgroups  = fetch_20newsgroups(subset='train') # Training subset of labelled data\n",
    "\n",
    "\n",
    "# Split Dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(data_newsgroups.data,data_newsgroups.target,test_size=0.3)\n",
    "\n",
    "# Build Classification pipeline\n",
    "\n",
    "svc_tfidf = \n",
    "\n",
    "scores = cross_val_score(svc_tfidf,X_train,y_train,cv=2).mean()\n",
    "print('Score',scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to data and get test predictions\n",
    "\n",
    "preds = \n",
    "\n",
    "# Evaluate the model and determine the metrics\n",
    "accuracy_score_ = accuracy_score(y_test,preds)\n",
    "classification_report_ = classification_report(y_test,preds)\n",
    "\n",
    "print('Accuracy Score', accuracy_score_)\n",
    "print('Classification Report',classification_report_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 2/4** - Latent Semantic Analysis(LSA) for Document Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use same dataset as above for document classification. But only consider 2 out of the 20 categories of newsgroups\n",
    "\n",
    "categories = ['talk.religion.misc','comp.graphics']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',categories=categories,remove=('headers','footers','quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',categories=categories,remove=('headers','footers','quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize and split data\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5,stop_words='english',use_idf=True,max_features=5000)\n",
    "X_train,X_test,y_train,y_test = train_test_split(newsgroups_train.data,newsgroups_train.target,test_size=0.3)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's project tfidf vectors to principle components and determine topics\n",
    "svd = \n",
    "lsa = \n",
    "\n",
    "# Project the training data to lower dimensions using SVD\n",
    "X_train_lsa = \n",
    "\n",
    "# Let's apply transformations to testing data\n",
    "X_test_tfidf = \n",
    "X_test_lsa = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build classifier model - e.g. KnnClassifier and fit to data\n",
    "knn_classify = KNeighborsClassifier()\n",
    "knn_classify.fit(X_train_lsa,y_train)\n",
    "\n",
    "# Get test set predictions and evaluate model metrics\n",
    "preds = knn_classify.predict(X_test_lsa)\n",
    "score = accuracy_score(y_test,preds)\n",
    "classify_report = classification_report(y_test,preds)\n",
    "print(score)\n",
    "print(classify_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 3/4** - Prepare Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now go back to the twitter dataset.\n",
    "\n",
    "DATA_PATH = \"COVID-19-Twitter-India/hourly_tweets/\"\n",
    "file_names_hourly = os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping Files From Hourly to Daily Basis\n",
    "file_names_daily = [file_name[:-7] for file_name in file_names_hourly]\n",
    "file_names_df = pd.DataFrame({'Hourly' : file_names_hourly, 'Daily': file_names_daily})\n",
    "file_names_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity any corrupt files \n",
    "def corrupt_or_not(file_name):\n",
    "    \"\"\"Some csv files are corrupt this is a program to spot them in DATA_PATH,\n",
    "    return : True if opens False for corrupt(not open)\"\"\"\n",
    "    try:\n",
    "        pd.read_csv(os.path.join(*[DATA_PATH,file_name]))\n",
    "        return False\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "file_names_df['Corrupt'] = file_names_df['Hourly'].apply(corrupt_or_not)\n",
    "file_names_df.groupby('Corrupt').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Corrupt Files\n",
    "file_names_df = file_names_df[file_names_df['Corrupt'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Groupby object to dict such that key is the day and values are the hourly file names\n",
    "file_daily_hourly_map = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_daily_hourly_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_frame(file_names):\n",
    "    \"Concatenates all dataframe from a day and returns dataframe after fixing the full_text column\"\n",
    "    hourly_df = [pd.read_csv(os.path.join(*[DATA_PATH,file_name])) for file_name in file_names]\n",
    "    daily_df = pd.concat(hourly_df)\n",
    "    daily_df = daily_df[(daily_df['full_text'] != 'No Value Mentioned') | (daily_df['full_retweet_text'] != 'No Value Mentioned')]\n",
    "    daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_text'] =  daily_df.loc[daily_df['full_text'] == 'No Value Mentioned','full_retweet_text']\n",
    "    daily_df['full_text'] = daily_df['full_text'].astype(str)\n",
    "    return daily_df\n",
    "\n",
    "\n",
    "final_df_tweets = pd.DataFrame()\n",
    "final_retweet_text_updated = []\n",
    "\n",
    "for key,file_names in tqdm(file_daily_hourly_map.items()):\n",
    "  final_df_tweets = \n",
    "  \n",
    "for f in list(final_df_tweets['full_retweet_text']):\n",
    "  t = type(f)\n",
    "  if f!='No Value Mentioned' and t==str:\n",
    "    final_retweet_text_updated.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 4/4** - Implement a topic model using gensim library and interpret document topic distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import ldamodel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_punctuation,strip_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all tweets\n",
    "final_corpus = final_retweet_text_updated\n",
    "\n",
    "# remove commond stopwords from each text in list of documents\n",
    "texts = [[word for word in document.lower().split() if word not in STOPWORDS]\n",
    "         for document in final_corpus]\n",
    "all_tokens = sum(texts,[])\n",
    "\n",
    "# remove duplicate tokens from set of words in each document of list\n",
    "tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)\n",
    "final_corpus = [[word for word in text if word not in tokens_once]\n",
    "         for text in texts]\n",
    "\n",
    "# make a bag of words corpus \n",
    "dictionary = \n",
    "corpus = \n",
    "\n",
    "# print out the documents and which is the most probable topics for each doc.\n",
    "lda = \n",
    "corpus_lda =\n",
    "\n",
    "\n",
    "lda_topics = \n",
    "\n",
    "topics = []\n",
    "filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]\n",
    "\n",
    "for topic in lda_topics:\n",
    "    print(topic)\n",
    "    topics.append(preprocess_string(topic[1], filters))\n",
    "\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of topics\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, corpus, dictionary=lda.id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce79a19f9cc0947084b77b508553b033dcdc8c0342682895062865bfd70db225"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('Code-gKRmiwwc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
