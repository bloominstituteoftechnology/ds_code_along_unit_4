{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "\n",
    "## **STEP: 0/4** - Install prerequisites and Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download spacy module\n",
    "\n",
    "%%time\n",
    "\n",
    "!python -m spacy download en_core_web_sm \n",
    "# If working on Colab, restart runtime after this step or else Colab won't find spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prerequesties\n",
    "!pip install -r requirements.txt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27 µs, sys: 6 µs, total: 33 µs\n",
      "Wall time: 33.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "Import Statements\n",
    "\"\"\"\n",
    "\n",
    "# Base\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the DATA_PATH variable\n",
    "\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  # If you're on Colab:\n",
    "  DATA_PATH = 'https://raw.githubusercontent.com/bloominstituteoftechnology/ds_code_along_unit_4/main/data/COVID-19-Twitter-India/'\n",
    "else:\n",
    "  # If you're working locally:\n",
    "  DATA_PATH = '..../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 1/4** - Tokenization and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Natural language processing (NLP) is a field \" + \\\n",
    "       \"of computer science, artificial intelligence \" + \\\n",
    "       \"and computational linguistics concerned with \" + \\\n",
    "       \"the interactions between computers and human \" + \\\n",
    "       \"(natural) languages, and, in particular, \" + \\\n",
    "       \"concerned with programming computers to \" + \\\n",
    "       \"fruitfully process large natural language \" + \\\n",
    "       \"corpora. Challenges in natural language \" + \\\n",
    "       \"processing frequently involve natural \" + \\\n",
    "       \"language understanding, natural language\" + \\\n",
    "       \"generation frequently from formal, machine\" + \\\n",
    "       \"-readable logical forms, connecting language \" + \\\n",
    "       \"and machine perception, managing human-\" + \\\n",
    "       \"computer dialog systems, or some combination \" + \\\n",
    "       \"thereof.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms, connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using using python (case normalization and regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tokens ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization using nltk\n",
    "Here we will see how to tokenize the text i.e. divide whole text into smaller chunks either chunks of sentences or chunks of words. There are two ways of tokenization i.e. sentence tokenization and word tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download punkt to help with tokenization\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are considering sample corpus for sentence tokenization\n",
    "\n",
    "tokenized_sents = \n",
    "tokenized_words = \n",
    "\n",
    "print(tokenized_sents,'\\n',tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words from a list of tokens\n",
    "Stopwords are the words which doesn't make that much sense when we play with sentences. For e.g the, has, have etc. So Let's remove it from list of tokens we generated above to cut down training time of building of machine learning model for downstream tasks.\n",
    "\n",
    "Let's import stopwords using nltk library and filter from list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of stop words\n",
    "stop_words = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of punctuations\n",
    "punctuation_list = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_words = stop_words + punctuation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenized_words = \n",
    "print(filtered_tokenized_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 2/4** - Lemmatization and Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "Stemming or lemmatization means generation of root form of inflected words. For e.g charge is derived by stemming or lemmatization of charging,charges etc.\n",
    "\n",
    "* There is little theoritical difference between these two i.e stemming may not result actual word but lemmatization will result actual word as root. So we can say that lemmatization is specialized form of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet') # WordNet is a large word database of English Nouns, Adjectives, Adverbs and Verbs. \n",
    "nltk.download('omw-1.4') # Multilingual WordNet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming using NLTK\n",
    "\n",
    "ps = PorterStemmer()\n",
    "words = ['Programs','Programming','Charging','Studying','Coding']\n",
    "for w in words:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using NLTK\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "words_ = ['rocks','programs','guests','games']\n",
    "for w in words_:\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count Vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = \n",
    "vectors = \n",
    "dtm =\n",
    "\n",
    "\n",
    "print('After vectorization')\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print vocabulary\n",
    "sorted(vectors.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df= pd.DataFrame(dtm.todense(), columns=vectors.get_feature_names_out()) # remember to convert sparse matrix to dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = \n",
    "tfidf_vectors = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfid_df = pd.DataFrame(tfidf_vectors.todense(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Features names', tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 3/4** - Working with documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use spacy pretrained word embeddings models trained on english wikipedia text\n",
    "nlp = spacy.load('en_core_web_sm') # here en_core_web_sm is small pretrained word embedding model, en_core_web_md -> medium size pretrained model and en_core_web_lg -> large size pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with all the tweet texts from the dataset \n",
    "data = pd.read_csv(DATA_PATH + 'tweets_2020-05-29-20.csv')\n",
    "docs = list(data['full_retweet_text'])\n",
    "\n",
    "docs = [str(tweet) for tweet in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 333 tweets to work with\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Spacy to Tokenize the tweets: Remove the stop words and the punctuation\n",
    "\n",
    "spacy_stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Initialize a list to hold the tokens\n",
    "all_doc_tokens = []\n",
    "\n",
    "# Loop over each tweet in the document (doc)\n",
    "for tweet in nlp.pipe(docs):\n",
    "    for token in tweet:\n",
    "\n",
    "\n",
    "    \n",
    "print(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Represent document as vector\n",
    "Let's say we have bunch of sentences in a document and we wanna do classify texts then how we can fed input to the machine learning model. As system understands numeric values, we need to convert text into numeric as feature vectors representation so that it can be fed to model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "vectors = count_vectorizer.fit(docs)\n",
    "dtm = vectors.transform(docs)\n",
    "\n",
    "\n",
    "print('After vectorization')\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(use_idf = True, analyzer = 'word', ngram_range=(1,2))\n",
    "tfidf_vectorizer.fit(docs)\n",
    "tfidf_vectors = tfidf_vectorizer.fit_transform(docs)\n",
    "print('TF-IDF Vectors', tfidf_vectors.toarray())\n",
    "print('Features names', tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## **STEP: 4/4** - Query Similar Documents and apply Word Embeddings Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query documents by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's consider first sentence in docs as query sentence, you can try selecting all tweets too\n",
    "\n",
    "query = \n",
    "similarity_matrix = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Word Embeddings Model to create Document Vectors\n",
    "\n",
    "Let's explore another method to create document vectors i.e. pretrained word embeddings models to get contextual features from given text sequences. These document vectors can help to classify sentences and many other downstream tasks further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here for each word in sentence particular value is being assigned as per spacy pretrained word embedding model\n",
    "embed_vectors = \n",
    "print(embed_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed = pd.DataFrame(embed_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ce79a19f9cc0947084b77b508553b033dcdc8c0342682895062865bfd70db225"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('Code-gKRmiwwc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
